{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXNKDE_BW-Kc"
   },
   "source": [
    "# YourTTS Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33efzXGOWDx3"
   },
   "source": [
    "## TTS Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79YIRAJiUdE9"
   },
   "source": [
    "### Download and install Coqui TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2akFqoi7UiD4",
    "outputId": "c27266cc-43b1-45d6-abe3-8318de65c7f7"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Edresson/Coqui-TTS -b multilingual-torchaudio-SE TTS\n",
    "# %pip install -q -e TTS/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaL5ju9sWPUO"
   },
   "source": [
    "### Download TTS Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yICxxOSZWYJb",
    "outputId": "a814b3d8-f12c-400c-9ab3-1284cd561efc"
   },
   "outputs": [],
   "source": [
    "# TTS checkpoints\n",
    "import os\n",
    "\n",
    "if not os.path.exists('best_model.pth.tar'):\n",
    "    # download config\n",
    "    ! gdown --id 1-PfXD66l1ZpsZmJiC-vhL055CDSugLyP\n",
    "    # download language json\n",
    "    ! gdown --id 1_Vb2_XHqcC0OcvRF82F883MTxfTRmerg\n",
    "    # download speakers json\n",
    "    ! gdown --id 1SZ9GE0CBM-xGstiXH2-O2QWdmSXsBKdC -O speakers.json\n",
    "    # download checkpoint\n",
    "    # ! gdown --id 1j1TuaCGTizpuHtKPNclkxtWamfpTT1Su -O best_model.pth.tar\n",
    "    ! gdown --id 1sgEjHt0lbPSEw9-FSbC_mBoOPwNi87YR -O best_model.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1sw5nDwXWRJ"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajwcjsizXYF9"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "TTS_PATH = \"TTS/\"\n",
    "\n",
    "# add libraries into environment\n",
    "sys.path.append(TTS_PATH) # set this if TTS is not installed globally\n",
    "\n",
    "import os\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from TTS.config import load_config\n",
    "from TTS.tts.models import setup_model\n",
    "from TTS.tts.models.vits import *\n",
    "from TTS.utils.audio import AudioProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vE3EDJyZXecO"
   },
   "source": [
    "### Paths definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8NWL6h2XiBP"
   },
   "outputs": [],
   "source": [
    "# model vars \n",
    "MODEL_PATH = 'best_model.pth.tar'\n",
    "CONFIG_PATH = 'config.json'\n",
    "TTS_LANGUAGES = \"language_ids.json\"\n",
    "TTS_SPEAKERS = \"speakers.json\"\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVTFLUVIacVG"
   },
   "source": [
    "### Restore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ygPFBC-UafNK",
    "outputId": "a2abb4fc-cd3e-490e-8f80-0a161e62da01"
   },
   "outputs": [],
   "source": [
    "# load the config\n",
    "C = load_config(CONFIG_PATH)\n",
    "\n",
    "\n",
    "# load the audio processor\n",
    "ap = AudioProcessor(**C.audio)\n",
    "\n",
    "speaker_embedding = None\n",
    "\n",
    "C.model_args['d_vector_file'] = TTS_SPEAKERS\n",
    "C.model_args['use_speaker_encoder_as_loss'] = False\n",
    "\n",
    "model = setup_model(C)\n",
    "model.language_manager.set_language_ids_from_file(TTS_LANGUAGES)\n",
    "cp = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "# remove speaker encoder\n",
    "model_weights = cp['model'].copy()\n",
    "for key in list(model_weights.keys()):\n",
    "  if \"speaker_encoder\" in key:\n",
    "    del model_weights[key]\n",
    "\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "# synthesize voice\n",
    "use_griffin_lim = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSgpKI-ZcVbx"
   },
   "source": [
    "##Speaker encoder setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3TaCzONgyND"
   },
   "source": [
    "### Install helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfnuqL4Zd4Zz"
   },
   "outputs": [],
   "source": [
    "# %pip install -q pydub ffmpeg-normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJkRPcD9g2nl"
   },
   "source": [
    "### Paths definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fRFfyZFeKuR"
   },
   "outputs": [],
   "source": [
    "CONFIG_SE_PATH = \"config_se.json\"\n",
    "CHECKPOINT_SE_PATH = \"SE_checkpoint.pth.tar\"\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_SE_PATH):\n",
    "    # download config \n",
    "    ! gdown --id  19cDrhZZ0PfKf2Zhr_ebB-QASRw844Tn1 -O $CONFIG_SE_PATH\n",
    "    # download checkpoint  \n",
    "    ! gdown --id   17JsW6h6TIh7-LkU2EvB_gnNrPcdBxt7X -O $CHECKPOINT_SE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYZ9YC_LhAjY"
   },
   "source": [
    "###Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnWyPs7Vfxa2"
   },
   "outputs": [],
   "source": [
    "from TTS.tts.utils.speakers import SpeakerManager\n",
    "from pydub import AudioSegment\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHUPL0TahHjZ"
   },
   "source": [
    "###Load the Speaker encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnkL2GNXhLUs"
   },
   "outputs": [],
   "source": [
    "SE_speaker_manager = SpeakerManager(encoder_model_path=CHECKPOINT_SE_PATH, encoder_config_path=CONFIG_SE_PATH, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc-OM81nhDDe"
   },
   "source": [
    "###Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRvXOFPKgVLi"
   },
   "outputs": [],
   "source": [
    "def compute_spec(ref_file):\n",
    "  y, sr = librosa.load(ref_file, sr=ap.sample_rate)\n",
    "  spec = ap.spectrogram(y)\n",
    "  spec = torch.FloatTensor(spec).unsqueeze(0)\n",
    "  return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KgvuzFRaql-"
   },
   "source": [
    "## TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjH5EW-E3jh1"
   },
   "source": [
    "### Upload, normalize and resample your reference wav files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KugzDPUbYtgM"
   },
   "source": [
    "Please upload wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_files = [f'../samples/speaker_man_korean/{index:>06d}.wav' for index in range(11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA-2jVOR3oCQ"
   },
   "source": [
    "### Compute embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ag_LhDHc2H61"
   },
   "outputs": [],
   "source": [
    "reference_emb = SE_speaker_manager.compute_d_vector_from_clip(reference_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boZL8mW03uWm"
   },
   "source": [
    "### Define inference variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtPraqy3augC"
   },
   "outputs": [],
   "source": [
    "model.length_scale = 1.0  # scaler for the duration predictor. The larger it is, the slower the speech.\n",
    "model.inference_noise_scale = 0.0 # defines the noise variance applied to the random z vector at inference.\n",
    "model.inference_noise_scale_dp = 0.0 # defines the noise variance applied to the duration predictor z vector at inference.\n",
    "# model.inference_noise_scale = 0.3 # defines the noise variance applied to the random z vector at inference.\n",
    "# model.inference_noise_scale_dp = 0.3 # defines the noise variance applied to the duration predictor z vector at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie4fU3Oz330K"
   },
   "source": [
    "### Chose language id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDgjXG0DbLWN"
   },
   "outputs": [],
   "source": [
    "model.language_manager.language_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdCvHScgbmim"
   },
   "outputs": [],
   "source": [
    "language_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9F1nRKo4Sgz"
   },
   "source": [
    "### Convert the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_speaker_encoder = torch.jit.trace(SE_speaker_manager.speaker_encoder, torch.randn(1, 129150))\n",
    "model_speaker_encoder.save(\"../speaker_encoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_text_inputs = torch.LongTensor(\n",
    "    [[164,  58, 164,  56, 164, 163, 164,  53, 164,  55, 164,  42, 164,  56,\n",
    "         164,  46, 164,  41, 164,  42, 164,  51, 164,  57, 164, 163, 164,  47,\n",
    "         164,  52, 164,  42, 164, 163, 164,  39, 164,  46, 164,  41, 164,  42,\n",
    "         164,  51, 164, 163, 164,  45, 164,  38, 164,  56, 164, 163, 164,  38,\n",
    "         164,  51, 164,  51, 164,  52, 164,  58, 164,  51, 164,  40, 164,  42,\n",
    "         164,  41, 164, 163, 164,  38, 164, 163, 164,  58, 164,  56, 164, 163,\n",
    "         164,  39, 164,  38, 164,  51, 164, 163, 164,  52, 164,  51, 164, 163,\n",
    "         164,  55, 164,  58, 164,  56, 164,  56, 164,  46, 164,  38, 164,  51,\n",
    "         164, 163, 164,  52, 164,  46, 164,  49, 164, 163, 164,  38, 164,  51,\n",
    "         164,  41, 164, 163, 164,  52, 164,  57, 164,  45, 164,  42, 164,  55,\n",
    "         164, 163, 164,  42, 164,  51, 164,  42, 164,  55, 164,  44, 164,  62,\n",
    "         164, 163, 164,  46, 164,  50, 164,  53, 164,  52, 164,  55, 164,  57,\n",
    "         164,  56, 164, 157, 164, 163, 164,  55, 164,  38, 164,  50, 164,  53,\n",
    "         164,  46, 164,  51, 164,  44, 164, 163, 164,  58, 164,  53, 164, 163,\n",
    "         164,  38, 164, 163, 164,  53, 164,  55, 164,  42, 164,  56, 164,  56,\n",
    "         164,  58, 164,  55, 164,  42, 164, 163, 164,  40, 164,  38, 164,  50,\n",
    "         164,  53, 164,  38, 164,  46, 164,  44, 164,  51, 164, 163, 164,  52,\n",
    "         164,  51, 164, 163, 164,  50, 164,  52, 164,  56, 164,  40, 164,  52,\n",
    "         164,  60, 164, 163, 164,  46, 164,  51, 164, 163, 164,  55, 164,  42,\n",
    "         164,  57, 164,  38, 164,  49, 164,  46, 164,  38, 164,  57, 164,  46,\n",
    "         164,  52, 164,  51, 164, 163, 164,  43, 164,  52, 164,  55, 164, 163,\n",
    "         164,  57, 164,  45, 164,  42, 164, 163, 164,  46, 164,  51, 164,  59,\n",
    "         164,  38, 164,  56, 164,  46, 164,  52, 164,  51, 164, 163, 164,  52,\n",
    "         164,  43, 164, 163, 164,  58, 164,  48, 164,  55, 164,  38, 164,  46,\n",
    "         164,  51, 164,  42, 164, 159, 164]]\n",
    ")\n",
    "dummy_text_inputs_lengths = torch.tensor(dummy_text_inputs.shape[1:2])\n",
    "\n",
    "dummy_sid = None\n",
    "dummy_g = torch.tensor([reference_emb]).unsqueeze(-1)\n",
    "dummy_lid = torch.tensor([language_id])\n",
    "\n",
    "args = (dummy_lid,)\n",
    "model_jit_emb_l = torch.jit.trace(\n",
    "    model.emb_l, args,\n",
    ")\n",
    "model_jit_emb_l.save('emb_l.pt')\n",
    "dummy_lang_emb = model_jit_emb_l(*args).unsqueeze(-1)\n",
    "\n",
    "args = (dummy_text_inputs, dummy_text_inputs_lengths, dummy_lang_emb,)\n",
    "model_jit_text_encoder = torch.jit.trace(\n",
    "    model.text_encoder, args,\n",
    ")\n",
    "model_jit_text_encoder.save('text_encoder.pt')\n",
    "dummy_x, dummy_m_p, dummy_logs_p, dummy_x_mask = model_jit_text_encoder(*args)\n",
    "\n",
    "args = (dummy_x, dummy_text_inputs_lengths, dummy_x_mask, torch.zeros(0), dummy_g, dummy_lang_emb, torch.tensor(True), torch.tensor(model.inference_noise_scale_dp))\n",
    "model_jit_duration_predictor_reversed = torch.jit.trace(\n",
    "    model.duration_predictor, args,\n",
    ")\n",
    "model_jit_duration_predictor_reversed.save('duration_predictor_reversed.pt')\n",
    "dummy_logw = model_jit_duration_predictor_reversed(*args)\n",
    "\n",
    "dummy_w = torch.exp(dummy_logw) * dummy_x_mask * model.length_scale\n",
    "dummy_w_ceil = torch.ceil(dummy_w)\n",
    "dummy_y_lengths = torch.clamp_min(torch.sum(dummy_w_ceil, [1, 2]), 1).long()\n",
    "dummy_y_mask = sequence_mask(dummy_y_lengths, dummy_y_lengths.data.max()).to(dummy_x_mask.dtype)\n",
    "dummy_attn_mask = torch.unsqueeze(dummy_x_mask, 2) * torch.unsqueeze(dummy_y_mask, -1)\n",
    "dummy_attn = generate_path(dummy_w_ceil.squeeze(1), dummy_attn_mask.squeeze(1).transpose(1, 2))\n",
    "\n",
    "dummy_m_p = torch.matmul(dummy_attn.transpose(1, 2), dummy_m_p.transpose(1, 2)).transpose(1, 2)\n",
    "dummy_logs_p = torch.matmul(dummy_attn.transpose(1, 2), dummy_logs_p.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "dummy_z_p = dummy_m_p + torch.randn_like(dummy_m_p) * torch.exp(dummy_logs_p) * model.inference_noise_scale\n",
    "\n",
    "args = (dummy_z_p, dummy_y_mask, dummy_g, torch.tensor(False),)\n",
    "model_jit_flow = torch.jit.trace(\n",
    "    model.flow, args,\n",
    ")\n",
    "model_jit_flow.save('flow.pt')\n",
    "\n",
    "args = (dummy_z_p, dummy_y_mask, dummy_g, torch.tensor(True),)\n",
    "model_jit_flow_reversed = torch.jit.trace(\n",
    "    model.flow, args,\n",
    ")\n",
    "model_jit_flow_reversed.save('flow_reversed.pt')\n",
    "dummy_z = model_jit_flow_reversed(*args)\n",
    "\n",
    "args = ((dummy_z * dummy_y_mask)[:, :, : model.max_inference_len], dummy_g,)\n",
    "model_jit_waveform_decoder = torch.jit.trace(\n",
    "    model.waveform_decoder, args,\n",
    ")\n",
    "model_jit_waveform_decoder.save('waveform_decoder.pt')\n",
    "dummy_o = model_jit_waveform_decoder(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_speaker_cond_src = torch.tensor([reference_emb])\n",
    "dummy_speaker_cond_tgt = torch.tensor([reference_emb])\n",
    "dummy_y = torch.randn(1, 513, 241)\n",
    "dummy_y_lengths = torch.tensor([dummy_y.size(2)])\n",
    "\n",
    "dummy_g_src = F.normalize(dummy_speaker_cond_src).unsqueeze(-1)\n",
    "dummy_g_tgt = F.normalize(dummy_speaker_cond_tgt).unsqueeze(-1)\n",
    "\n",
    "args = (dummy_y, dummy_y_lengths, dummy_g_src,)\n",
    "model_jit_posterior_encoder = torch.jit.trace(\n",
    "    model.posterior_encoder, args,\n",
    ")\n",
    "model_jit_posterior_encoder.save('posterior_encoder.pt')\n",
    "dummy_z, _, _, dummy_y_mask = model_jit_posterior_encoder(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_text_inputs = torch.LongTensor(\n",
    "    [[164,  58, 164,  56, 164, 163, 164,  53, 164,  55, 164,  42, 164,  56,\n",
    "         164,  46, 164,  41, 164,  42, 164,  51, 164,  57, 164]]\n",
    ")\n",
    "dummy_text_inputs_lengths = torch.tensor(dummy_text_inputs.shape[1:2])\n",
    "\n",
    "dummy_sid = None\n",
    "dummy_g = torch.tensor([reference_emb]).unsqueeze(-1)\n",
    "dummy_lid = torch.tensor([language_id])\n",
    "\n",
    "args = (dummy_lid,)\n",
    "dummy_lang_emb = model_jit_emb_l(*args).unsqueeze(-1)\n",
    "\n",
    "args = (dummy_text_inputs, dummy_text_inputs_lengths, dummy_lang_emb,)\n",
    "dummy_x, dummy_m_p, dummy_logs_p, dummy_x_mask = model_jit_text_encoder(*args)\n",
    "\n",
    "args = (dummy_x, dummy_text_inputs_lengths, dummy_x_mask, torch.zeros(0), dummy_g, dummy_lang_emb, torch.tensor(True), torch.tensor(model.inference_noise_scale_dp))\n",
    "dummy_logw = model_jit_duration_predictor_reversed(*args)\n",
    "\n",
    "dummy_w = torch.exp(dummy_logw) * dummy_x_mask * model.length_scale\n",
    "dummy_w_ceil = torch.ceil(dummy_w)\n",
    "dummy_y_lengths = torch.clamp_min(torch.sum(dummy_w_ceil, [1, 2]), 1).long()\n",
    "dummy_y_mask = sequence_mask(dummy_y_lengths, dummy_y_lengths.data.max()).to(dummy_x_mask.dtype)\n",
    "dummy_attn_mask = torch.unsqueeze(dummy_x_mask, 2) * torch.unsqueeze(dummy_y_mask, -1)\n",
    "dummy_attn = generate_path(dummy_w_ceil.squeeze(1), dummy_attn_mask.squeeze(1).transpose(1, 2))\n",
    "\n",
    "dummy_m_p = torch.matmul(dummy_attn.transpose(1, 2), dummy_m_p.transpose(1, 2)).transpose(1, 2)\n",
    "dummy_logs_p = torch.matmul(dummy_attn.transpose(1, 2), dummy_logs_p.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "dummy_z_p = dummy_m_p + torch.randn_like(dummy_m_p) * torch.exp(dummy_logs_p) * model.inference_noise_scale\n",
    "\n",
    "args = (dummy_z_p, dummy_y_mask, dummy_g, torch.tensor(True),)\n",
    "dummy_z = model_jit_flow_reversed(*args)\n",
    "\n",
    "args = ((dummy_z * dummy_y_mask)[:, :, : model.max_inference_len], dummy_g,)\n",
    "dummy_o = model_jit_waveform_decoder(*args)\n",
    "\n",
    "wav = dummy_o.detach().double().numpy()[0, 0]\n",
    "ap.save_wav(wav, '../../output-tts.wav')\n",
    "\n",
    "wav.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "33efzXGOWDx3",
    "uSgpKI-ZcVbx"
   ],
   "name": "YourTTS-zeroshot-TTS-demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "c5769a634673281e8d552b893f0d7b4d4643e41bb657b53c02f9ea02e9f1a875"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
